# **数据预处理**
## 1 综述
### 1.1 归一化和KD树：
#### 1.1.1 归一化
- 原因：在量纲不同的情况下，不能反映样本中每一个特征的重要程度。
- 解决方法：把所有的数据都映射到同一个尺度（量纲）上。
- 两种归化一方法：
    - **最值归一化（Min-Max scaling）**（把数据映射到（0，1）之间，受极端值的影响）
        - 缺点：
            - 当有新数据加入时，可能导致max和min的变化，需要重新定义
            - MinMaxScaler对异常值的存在非常敏感。
        ```math
        x_{scal} = \frac{x-x_{min}}{x_{max}-x_{min}}
        ```

    - **均值方差归一化(Z-score standardization)**（把所有数据归一到均值为0方差为1的分布中，数据不一定在0-1之间，受极端值的影响较小。比较适用于数据分布近似高斯分布的数据集，否则效果不太好。）
        - 优点：
            - 简单、容易计算。
            - 不受数据量级的影响。
            - Z-Score能够应用于数值型的数据。
        - 缺点：
            - 真实的分析与挖掘中很难得到平均值与方差，大多数情况下是用样本的均值与标准差替代。
            - Z-Score对于数据的分布有一定的要求。正态分布是最有利于Z-Score计算的。
            - Z-Score消除了数据具有的实际意义、只能用于比较数据间的结果。
            - 在存在异常值时无法保证平衡的特征尺度。
        ```math
        x_{scal} = \frac{x-x_{mean}}{S}
        ```
    - **MaxAbs归一化**（把所有数据归一到均值为0方差为1的分布中，数据不一定在0-1之间，受极端值的影响较小。比较适用于数据分布近似高斯分布的数据集，否则效果不太好。）
    -  缺点：
        -  这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。
        -  MaxAbsScaler与先前的缩放器不同，绝对值映射在[0,1]范围内。
在仅有正数据时，该缩放器的行为MinMaxScaler与此类似，因此也存在大的异常值。
        ```math
        x^{'} = \frac{x}{|Max|}
        ```

    - **正态分布化（Normalization）**
        -   Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。

```math
x = \frac{x}{\sqrt{\sum_{j}^{d}}(x_j)^2}
```

-  问题：
    -  标准化与归一化的异同：
        -  相同点：
            - 都能取消由于量纲不同引起的误差；
            - 都是一种线性变换；
            - 都是对向量X按照比例压缩再进行平移。
        -  不同：
            - 目的不同。归一化是为了消除纲量压缩到[0,1]区间；标准化只是调整特征整体的分布。
            - 归一化与最大，最小值有关；标准化与均值，标准差有关。
            - 归一化输出在[0,1]之间；标准化无限制。
    -  什么时候用归一化？什么时候用标准化？
        -  如果对输出结果范围有要求，用归一化。
        -  如果数据较为稳定，不存在极端的最大最小值，用归一化。
        -  如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。
    -  归一化与标准化的应用场景
        -  在分类、聚类算法中，需要使用距离来度量相似性的时候（如SVM、KNN）、或者使用PCA技术进行降维的时候，标准化(Z-score standardization)表现更好；
        -  在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。
        -  基于树的方法不需要进行特征的归一化、ICA(独立成分分析)、基于平方损失的最小二乘法OLS同样。
        -  一般来说，建议优先使用标准化。对于输出有要求时再尝试别的方法，如归一化或者更加复杂的方法。很多方法都可以将输出范围调整到[0, 1]，如果我们对于数据的分布有假设的话，更加有效的方法是使用相对应的概率密度函数来转换。
    - 训练数据集需要进行归一化、测试集不需要进行归一化
    -  优点：
        -  提升模型的收敛速度；例如：梯度下降。
        -  提升模型的精度；例如：降低权值影响。
        -  深度学习中数据归一化可以防止模型梯度爆炸。
。


#### 1.1.2 KD-Tree
- kd—Tree构建：
    - 循环依序取数据点的各维度来作为切分维度。
    - 取数据点在该维度的中值作为切分超平面。
    - 将中值左侧的数据点挂在其左子树，将中值右侧的数据点挂在其右子树，
    - 递归处理其子树，直至所有数据点挂载完毕。
- 优化点：
    - 选择切分维度：方差越大，分布越分散，从方差大的维度开始切分，有较好的切分效果和平衡性。
    - 确定中值点：预先对原始数据点在所有维度进行一次排序，存储下来，然后在后续的中值选择中，无须每次都对其子集进行排序，提升了性能。也可以从原始数据点中随机选择固定数目的点，然后对其进行排序，每次从这些样本点中取中值，来作为分割超平面。该方式在实践中被证明可以取得很好性能及很好的平衡性。
- kd—Tree的检索：
    - 给定一个点，从树的根节点开始遍历，计算与根节点的左右节点的距离后与根节点比较，选其小的，继续遍历。直到叶子上。

#### 1.1.3 数值型特征特征分箱（数据离散化）
- 分箱的优势：
    - 离散特征的增加和减少都很容易，易于模型的快速迭代。
    - 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。
    - 离散化后的特征对异常数据有很强的鲁棒性。年龄30岁和300岁，离散化后，影响较小。
    - 对于线性模型，表达能力受限。
        - 单变量离散化为N个后，每个变量有单独的权重，相当于模型引入了非线性，能够提升模型表达能力，加大拟合。
    - 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。
    - 特征离散化后，模型会更稳定。
    - 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
    - 可以将缺失作为独立的一类带入模型。
    - 将所有变量变换到相似的尺度上。
- 分箱后需要进行特征编码，如：LabelEncode、OneHotEncode或LabelBinarizer等。
##### 1.1.3.1 无监督分箱法
1. 自定义分箱（根据业务经验和常识）
2. 等距分箱（根据一定的距离划分。受异常值影响。）
3. 等频分箱（每份的样本数相等。）
4. 聚类分箱（利用k-mean聚类，保证第一类小于第二类。）
    1. 对预处理后的数据进行归一化处理。
    2. 将归一化处理过的数据，应用k-means聚类算法，划分为多个区间。采用等距法设定k-means聚类算法的初始中心，得到聚类中心。
    3. 在得到聚类中心后将相邻的聚类中心的中点作为分类的划分点，将各个对象加入到距离最近的类中，从而将数据划分为多个区间。
    4. 重新计算每个聚类中心，然后重新划分数据，直到每个聚类中心不再变化，得到最终的聚类结果。
5. 二值化（Binarization）
##### 1.1.3.2 有监督分箱法
1. 卡方分箱法
    -  定义：自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。
    -  基本思想：对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。
    -  步骤：
        -  预先定义一个卡方的阈值。
        -  初始化。根据要离散的属性对实例进行排序，每个实例属于一个区间。
        -  合并区间。（计算每一对相邻区间的卡方值、将卡方值最小的一对区间合并。）
        ```math
        X^2=\sum_{i=1}^{2}\sum_{j=1}^{2}\frac{(A_{ij}-E_{ij})^2}{E_{ij}}
        ```
        符号 | 描述 
        :-:|:-:
        A<sub>ij</sub>|第i区间第j类的实例的数量
        E<sub>ij</sub>|A<sub>ij</sub>的期望频率A<sub>ij</sub>=(N<sub>i</sub>*C<sub>j</sub>)/N
        N|总样本数
        N<sub>i</sub>|第i组的样本数
        C|第j类样本在全体中的比例
    -   阈值的意义:类别和属性独立时,有90%的可能性,计算得到的卡方值会小于4.6。大于阈值4.6的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大,区间合并就会进行很多次,离散后的区间数量少、区间大。      
    -  注意:
        -  ChiMerge算法推荐使用0.90、0.95、0.99置信度，最大区间数取10到15之间。
        -  也可以不考虑卡方阈值，此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限，最多几个区间,最少几个区间。
        -  对于类别型变量，需要分箱时需要按照某种方式进行排序。
2. 最小熵法分箱（需要使总熵值达到最小，也就是使分箱能够最大限度地区分因变量的各类别。）

**参考链接：https://mp.weixin.qq.com/s/qWO9zgKyntvyWfftpGqrHQ**


### 1.2 特征预处理：
- 特征工程：
    - Data PreProcessing（数据预处理）
    - Feature Extraction（特征提取）
    - Feature Selection（特征选择）
    - Feature construction（特征构造）
    - ……
- 数据预处理:
    - 数据清洗
    - 特征预处理
    - ……

## 2 代码实现
文件名 | 描述 
:-:|:-:
01_Min-Max-Zscore.py|MinMax、Z-score手写代码
02_standard_toclass.py|Z-score代码封装
03_kd_tree.py|kd-Tree的Demo
04_bining.py|分箱的例子
05_from_sklearn.py|sklearn数据归一化方法