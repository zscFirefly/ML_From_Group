# **线性回归**
## 1 综述
### 1.1 线性回归概述：
- **什么是简单线性回归？**
    - 简单：只有一个样本特征，即一个自变量。
    - 线性：方程是线性的。
    - 回归：用方程来模拟变量之间是如何关联的。
- **什么是损失函数?**
    - 损失函数是衡量预测模型预测期望结果表现的指标。损失函数越小，模型的鲁棒性越好。
    - **常用的损失函数：**
        - 0-1损失函数：用来应用分类问题。当预测分类错误时，损失函数值为1，正确为0。
        ```math
        L(Y,f(x))=
        \begin{cases}
        1, Y\not=f(x) \\
        0, Y = f(x)
        \end{cases}
        ```
        - 平方损失函数：用来描述回归问题，用来表示连续性变量，为预测值与真实值差值的平方。（误差值越大、惩罚力度越强，也就是对差值敏感）
        ```math
        L(Y,f(x))=(Y-f(X))^2
        ```
        - 平方损失函数：用来描述回归问题，用来表示连续性变量，为预测值与真实值差值的平方。（误差值越大、惩罚力度越强，也就是对差值敏感）
        ```math
        L(Y,f(x))=|Y-f(X)|
        ```
        - 对数损失函数：是预测值Y和条件概率之间的衡量。事实上，该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。
        ```math
        L(Y,f(x))=-logP(Y|X)
        ```
    - **期望风险：** 期望风险是损失函数的期望。
    - **经验风险：** 模型f(X)关于训练数据集的平均损失。**（只考虑经验风险的话，会出现过拟合现象，因此需要引出结构风险。）**
    - **结构风险：** 经验风险+正则项，防止过拟合的策略。（正则项：函数的复杂度再乘一个权重系数）
    - **经验风险和期望风险的关系：**
        - 经验风险是局部的，基于训练集所有样本点损失函数最小化。经验风险是局部最优，是现实的可求的。
        - 期望风险是全局的，基于所有样本点损失函数最小化。期望风险是全局最优，是理想化的不可求的。
        - 期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本数据集的平均损失。根据大数定律，当样本容量N趋于无穷时，经验风险趋于期望风险。
### 1.2 线性回归推导：
#### 1.2.1 问题
假设X=[x1,x2...xn]，Y=[y1,y2...yn]，求回归方程。
#### 1.2.2 推导
假设回归方程为：y=ax+b。对于所有的x<sub>i</sub>，有y<sub>i</sub>'。
因此，利用平方损失误差，可求得损失函数
```math
\begin{aligned}
Z &=\sum^{m}_{i=1}(ax_{i}+b-y_{i})^2\\
&= (\sum^{m}_{i =1}y'_i-\sum^{m}_{i =1}y_i)^2\\
&=(\sum^{m}_{i =1}ax_{i}+mb-\sum^{m}_{i =1}y_i)^2\\
\end{aligned}
```
因此，问题转化为求损失函数Z的最小值下，a和b的值。
对a和b分别求导，并令导数结果为0。

对b求导计算，如下：
```math
\frac{\partial Z} {\partial b} = 2(\sum^{m}_{i =1}ax_{i}+mb-\sum^{m}_{i =1}y_{i})=0

(\sum^{m}_{i =1}y_{i}-\sum^{m}_{i =1}ax_{i})=mb

(\frac{1}{m}\sum^{m}_{i =1}y_{i}-\frac{1}{m}\sum^{m}_{i =1}ax_{i})=b

\bar{y}-a\bar{x}_{i}=b
```
对a求导，将b值代入原式得如下：
```math
\begin{aligned}
\frac{\partial Z} {\partial a} &= \sum^{m}_{i=1}2(ax_{i}+b-y_{i})(x_{i})\\
&=\sum^{m}_{i=1}2(ax_{i}+\bar{y}-a\bar{x}_{i}-y_{i})(x_{i})\\
&= \sum^{m}_{i=1}2(ax_{i}^{2}+x_{i}\bar{y}-a\bar{x}_ix_{i}-x_{i}y_{i})\\
&=0
\end{aligned}
```
化简：
```math
\begin{aligned}
\sum^{m}_{i=1}2(ax_{i}^{2}+x_{i}\bar{y}-a\bar{x}_ix_{i}-x_{i}y_{i}) &=0\\
\sum^{m}_{i=1}(ax_{i}^{2}-a\bar{x}x_{i})+\sum^{m}_{i=1}(x_{i}\bar{y}-x_{i}y_{i})&=0\\
\sum^{m}_{i=1}a(x_{i}^{2}-\bar{x}x_{i})&=\sum^{m}_{i=1}(x_{i}y_{i}-x_{i}\bar{y})\\
a &= \frac{\sum^{m}_{i=1}(x_{i}y_{i}-x_{i}\bar{y})} {\sum^{m}_{i=1}(x_{i}^{2}-\bar{x}x_{i})}\\
a &= \frac{\sum^{m}_{i=1}(x_{i}y_{i}-x_{i}\bar{y}-x_{i}y_{i}+x_{i}\bar{y})} {\sum^{m}_{i=1}(x_{i}^{2}-\bar{x}x_{i}+x_{i}^{2}-\bar{x}x_{i})}\\
a &= \frac{\sum^{m}_{i=1}(x_{i}y_{i}-x_{i}\bar{y}-\bar{x}y_{i}+\bar{x}\bar{y})} {\sum^{m}_{i=1}(x_{i}^{2}-\bar{x}x_{i}+\bar{x}^{2}-\bar{x}x_{i})}\\
a &= \frac{\sum^{m}_{i=1}(x_{i}-\bar{x})(y_{i}-\bar{y})} {\sum^{m}_{i=1}(x_{i}-\bar{x})^2}
\end{aligned}
```
### 1.3 多元线性回归：
#### 1.3.1 问题
对于下面的样本数据集
```math
x^{i} = (X_{1}^{(i)},X_{2}^{(i)}...X_{3}^{(i)})
```
对应的是一个向量，每一行是一个样本，每列对应一个特征。求解多元线性方程。
#### 1.3.2 结论
解方程：
```math
\hat{y}=X_{b}\theta
```
损失函数：
```math
loss = (y-X_{b}\theta)(X_{b}\theta)
```
多元线性回归的正规方程解：
```math
\theta=(X_{b}^{T}X_{b})^{-1}X_{b}^{T}y
```

## 2 代码实现
文件名 | 描述 
:-:|:-:
01_LinearRegression_mycode.py|手写最小二乘法代码
02_vector_caculate_rate.py|向量化运算速度测试
03_LinearRegression_toclass.py.py|线性回归手写封装代码
04_muti_LinearRegression_toclass.py|多元线性回归封装代码（可不看，内涵知识点）
05_muti_LinearRegression_fromsklearn.py|多元线性回归sklearn实现

- python函数：
```python
np.hstack(tup): 按列顺序把数组给堆叠起来。
>> np.hstack([[1,2],[2,2]])
array([1, 2, 2, 2])

np.linalg.inv(): 计算矩阵的逆。
>>> np.linalg.inv([[1,2],[2,1]])
array([[-0.33333333,  0.66666667],
       [ 0.66666667, -0.33333333]])
       
np.linalg.inv(): 生成全0矩阵。
>>> np.ones((2,3),int)
array([[1, 1, 1],
       [1, 1, 1]])
     
T：array的方法，对矩阵进行转置。
>>> np.array([[1,1],[2,3]]).T
array([[1, 2],
       [1, 3]])

dot：点乘
A=np.array([[1,2],[1,1]])
B=np.array([[1,2],[3,4]])
A.dot(B)
```
