# **其他及其学习重要概念**
## 1 综述
### 1.1 多项式回归
- 什么是多项式回归？
    - 他使用线性回归的思路，为数据添加新的特征（新的特征是原有的特征的多项式组合），来解决非线性问题。
    - 例如：
        - 二元二次多项式回归，特征有`$x_{1},x_{2},x_{1}^{2},x_{2}^{2},x_{1}x_{2},b$`六项。
        - 二元三次多项式回归，特征有`$x_{1}^{3},x_{2}^{3},x_{1}^{2}x_{2},x_{1}x_{2}^{2},x_{1}^{2},x_{2}^{2},x_{1}x_{2},x_{1},x_{2},b$`十项。
### 1.2 偏差（Bias）和方差（Variance）
- 什么是Bias？什么是Variance？
    - 偏差衡量了模型的预测值与实际值之间的偏离关系。
    - 训练数据在不同迭代阶段的训练模型中，预测值的变化波动情况（或称之为离散情况）
- 偏差和方差的四种情况？
    - **低偏差，低方差**：这是训练的理想模型。
    - **低偏差，高方差**：过拟合。
    - **高偏差，低方差**：欠拟合，训练的初始阶段。
    - **高偏差，高方差**：训练最糟糕程度。
- 什么是模型误差？
    - 偏差 + 方差 + 不可避免的误差（噪音）。
    - 高偏差，低方差
    - 低偏差，低方差
    - 低偏差，高方差
- 偏差和方差的原因？
    - 偏差：主要的原因可能是对问题本身的假设是不正确的，或者欠拟合。
    - 方差：模型太过于复杂，模型没有完全学习到问题的本质，而学习到很多噪音。
    - 高方差算法：非参数学习算法通常都是高方差，因为对数据没有进行假设（如knn）
    - 高偏差算法：参数学习算法通常都是高偏差算法。（如线性回归）
- 处理高方差的手段：
    - 降低模型复杂度；
    - 减少数据纬度；
    - 降噪；
    - 增加样本数；
    - 使用验证集；
    - 模型正则化；
### 1.3 正则化
#### 1.3.1 L1正则
#### 1.3.1.1 LASSO回归思路
损失函数修改如下
```math
J(\theta)=MSE(y,\hat{y};\theta)+\alpha\sum_{i=1}^{n}|\theta_{i}|
```
- **注意：**
    - `$\sum_{i=1}^{n}\theta_{i}$`取值范围1～n，因为`$\theta_{0}$`本身就是一个截距。反应到图形上就是反应曲线到高低，不反应陡与缓。
    - `$\alpha$`表示优化的侧重点。要让每个`$\theta_{i}$`小的程度占整个优化损失函数程度的多少。
#### 1.3.1.2 L1正则化与稀疏性
- 什么是稀疏性？
    - 模型的很多参数是0，也就是模型为0的参数对应的特征对模型没有影响。可以作为特征选择的方法。
- 图文解释：略。

### 1.3.2 L2正则
#### 1.3.2.1 岭回归思路
损失函数修改如下
```math
J(\theta)=MSE(y,\hat{y};\theta)+\alpha\sum_{i=1}^{n}\theta_{i}^{2}
```
**注意：**
- L2范数能让解比较小（靠近0），但是比较平滑（不等于0）且不具有稀疏性。

### 1.3.3 L1和L2总结
- `$\alpha$`取值过大反而会导致误差增加，拟合曲线为直线。
- L1可以做特征筛选。
- L2可能会去除掉正确的特征，从而降低准确度。
- L1正则化在损失函数后面加L1范数，加上L1范数容易得到稀疏解。
- L2正则化在损失函数后面加L2范数，加上L2的解比较平滑，但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。

## 2 代码实现
文件名 | 描述 
:-:|:-:
01_multi_regression.py|多元项回归和pipline
02_l1_and_l2.py|L1和L2正则化调整参
