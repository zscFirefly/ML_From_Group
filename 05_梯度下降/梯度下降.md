# **梯度下降法**
## 1 综述
### 1.1 梯度下降概述：
- **什么是批量梯度下降（BGD）？**
    - 批量梯度下降是一种基于搜索的最优化方法。他是对原始模型的损失函数进行优化，以便寻找到最优的参数，使得损失函数的值最小。
- **什么是梯度？** 
    -  多元函数的导数就是梯度(gradient)。分别对每个变量进行微分，然后用逗号分割开，梯度是用括号包括起来。因此，梯度是一个向量。**(在单变量函数中，梯度就是函数的微分)**
```math
\nabla L = (\frac{\partial L}{\partial a},\frac{\partial L}{\partial b})
```
- **什么是梯度方向？**
    - 梯度的方向就指出了函数在给定点的上升最快的方向。**(由于我们要求的是误差最小点，故需要沿负梯度方向。以一元二次函数做思考方向。)**

### 1.2 推导过程：
#### 1.2.1 一元推导过程：
已知泰勒一阶展式
```math
f(\theta) \approx f(\theta_{0}) + (\theta-\theta_{0}) \nabla f(\theta_{0})\\
(\nabla f(\theta_{0}):函数f(\theta_{0})在\theta_{0}处的导数)
```
设`$\theta-\theta_{0}$`的单位向量为`$v$`,`$\eta$`标量，即步长。故有`$\theta-\theta_{0}=\eta v$`.

因此有`$f(\theta)\approx f(\theta)+\eta v \nabla f(\theta_{0})$`.

由于我们希望每一步更新后`$f(\theta)<f(\theta_{0})$`(因为我们需要使误差逐渐变小)。

因此 `$f(\theta)-f(\theta_{0})<0$`,即`$\eta v \nabla f(\theta_{0})<0$`.

因此`$v$`和`$\nabla f(\theta_{0})$`两个向量乘积小于0。

故两者方向应该相反。

为使梯度下降最快，故两个向量乘积为-1时，值最大。

因此`$v$`为`$\nabla f(\theta_{0})$`的反方向。

#### 1.2.2 扩展：

由上述推导到多维情况，由泰勒展式有移动步长
```math
\Delta L=L(a_{1},b_{1})-L(a_{0},b_{0})\approx \frac{\partial L}{\partial a}\Delta a + \frac{\partial L}{\partial b}\Delta b  \\
其中：\frac{\partial L}{\partial a},\frac{\partial L}{\partial b}为方向，\Delta a，\Delta b为步长。
```

为使`$\Delta L < 0$`(因为我们需要使误差逐渐变小)。

故设`$\Delta a=-\eta\frac{\partial L}{\partial a},\Delta b=-\eta\frac{\partial L}{\partial b}$`,其中`$\eta$`为步长。

因此我们就可以得到损失函数值（也就是下一步的落脚点）的迭代公式：
```math
(a_{k+1},b_{k+1}) = (a_{k}-\eta \frac{\partial L}{\partial a},b_{k}-\eta \frac{\partial L}{\partial b})
```

#### 1.2.3 多元回归梯度推导：
已知方程`$y=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+...+\theta_{n}x_{n}$`,令`$x_{0}=1$`，则`$y=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+...+\theta_{n}x_{n}$`。
因此，损失函数
```math
J=\sum_{i=1}^{N}(y-\theta_{0}x_{0}-\theta_{1}x_{1}-\theta_{2}x_{2}-...-\theta_{n}x_{n})^2
```
上式`$J$`对`$\theta_{0}$`求偏导，有
```math
\frac{\partial J}{\partial \theta_{0}}
= 2\sum_{i=1}^{N}(y-\theta_{0}x_{0}-\theta_{1}x_{1}-\theta_{2}x_{2}-...-\theta_{n}x_{n})(-x_{0})
```
因为m样本越多，损失越大，故需要对损失函数求均值，即
```math
\frac{\partial J}{\partial \theta_{0}}
= \frac{2}{N}\sum_{i=1}^{N}(y-\theta_{0}x_{0}-\theta_{1}x_{1}-\theta_{2}x_{2}-...-\theta_{n}x_{n})(-x_{0})
```
即，简写为：
```math
\frac{\partial J}{\partial \theta_{0}}
= \frac{2}{N}\sum_{i=1}^{N}(\theta x - y)(x_{0})
```
其中`$\theta$`为行向量，`$x$`为列向量。

因此对所有`$\theta$`求偏导，构成该点梯度：
```math
\frac{\nabla J}{\nabla \theta}=
\left[
    \begin{matrix}
        \frac{\partial J}{\partial \theta_{0}}
        = \frac{2}{N}\sum_{i=1}^{N}(\theta x - y)(x_{0})  \\
        \frac{\partial J}{\partial \theta_{1}}
        = \frac{2}{N}\sum_{i=1}^{N}(\theta x - y)(x_{1})  \\
        ...
        \\
        \frac{\partial J}{\partial \theta_{n}}
        = \frac{2}{N}\sum_{i=1}^{N}(\theta x - y)(x_{n}) 
    \end{matrix}
\right] 
```
若此时估计的`$\theta$`为：
```math
\theta = 
\left[
    \begin{matrix}
        \theta_{0}\\
        \theta_{1}\\
        ...\\
        \theta_{n}\\
    \end{matrix}
\right] 
```
若学习率为`$\eta$`，则下次迭代的`$\theta_{next}$`
```math
\theta_{next} = \theta_{now} - \eta \frac{\nabla J}{\nabla \theta} =
\left[
    \begin{matrix}
        \theta_{0}\\
        \theta_{1}\\
        ...\\
        \theta_{n}\\
    \end{matrix}
\right] 
- \eta
\left[
    \begin{matrix}
        \frac{\partial J}{\partial \theta_{0}}
        = \frac{2}{N}\sum_{i=1}^{N}(\theta x - y)(x_{0})  \\
        \frac{\partial J}{\partial \theta_{1}}
        = \frac{2}{N}\sum_{i=1}^{N}(\theta x - y)(x_{1})  \\
        ...
        \\
        \frac{\partial J}{\partial \theta_{n}}
        = \frac{2}{N}\sum_{i=1}^{N}(\theta x - y)(x_{n}) 
    \end{matrix}
\right] \tag{*}
```

### 1.3 推导过程中常见问题：
- 为什么要梯度要乘以一个负号？
    - 梯度的方向就是损失函数值在此点上升最快的方向，但我们要使损失最小，因此就要逆着梯度方向走，所以此处需要加上负号。
- 关于学习率：
    - 学习率过大：迈过最低点，出现钟摆，导致不收敛。
    - 学习率过小：收敛过慢，算法效率较低。
- 梯度下降的致命问题：
    - 从理论上，它只能保证达到局部最低点，而非全局最低点。
    - 解决方案：
        -  随机产生多个初始参数集，分别使用梯度下降直至各自收敛，再从收敛结果取最小值。
- 梯度下降的其他问题： 
    - 由于(*)式可知，在梯度下降中，需要对每个样本求梯度求和再取均值，这样导致训练速度大幅度下降。
    - 解决方案：
        - 随机梯度下降（SGD），每次只取一个样本点，求得一个方向，虽然不是每次迭代得到的损失函数都向着全局最优方向，但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。
        - 这是一个精度换时间的问题。
        - 随机梯度下降法的过程中，学习率必须逐渐衰减，避免到了最小值附近的时候，跳出去了。
- SGD和BGD的对比：
    - BGD优点：全局最优解；易于并行实现
    - BGD缺点：当样本数据很多时，计算量开销大，计算速度慢。
    - SGD优点：计算速度快；
    - SGD缺点：性能不好；
    - 结合两者的优缺点，这种产生了Mini-Batch Gradient Descent（小批量梯度随机下降）

## 2 代码实现
文件名 | 描述 
:-:|:-:
01_LinearRegression_mycode.py|python中两种求导方法的实现
02_vector_caculate_rate.py|手写批量梯度下降代码
03_LinearRegression_toclass.py.py|梯度下降代码封装
04_muti_LinearRegression_toclass.py|多元线性回归与批量梯度下降封装、并且和SGD中sklearn中的性能对比
05_muti_LinearRegression_fromsklearn.py|SGD和BGD的性能对比

## 3 参考链接
- 梯度下降介绍：https://mp.weixin.qq.com/s/44p8anqiiQV6XYGqH5u-Ug
- 梯度下降代码实现：https://mp.weixin.qq.com/s/nI9IBa4ccfg0xqyn0tbRPA
- 多元线性回归梯度下降：https://mp.weixin.qq.com/s/8gStYSSBvkXeuaX6Pp9qiQ
- 多元线性回归梯度下降（详细）：https://blog.csdn.net/weixin_44344462/article/details/88989450
- 随机梯度下降：https://mp.weixin.qq.com/s/OUslRwKGpS29gncsiyAPyg